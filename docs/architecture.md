# Akari プロジェクトアーキテクチャ概要

## 導入

Akariプロジェクトは、柔軟性と拡張性を重視したモジュラー型アプリケーションフレームワークです。音声認識、自然言語処理、外部API連携など、様々な機能をモジュールとして組み合わせ、パイプライン処理を容易に構築することを目的としています。

## 主要コンポーネント/モジュール

Akariプロジェクトは、主に以下のコンポーネント/モジュールで構成されています。

1.  **`akari` ディレクトリ**: フレームワークのコア機能を提供します。
    *   `__init__.py`: Akariフレームワークの主要なクラスや型を公開します。
    *   `data.py`: パイプライン内でモジュール間を流れるデータ構造（`AkariData`, `AkariDataSet`など）を定義します。データの型付けやストリーム処理の基盤となります。
    *   `logger.py`: 標準化されたロギング機能を提供し、アプリケーションの動作記録やデバッグを支援します。
    *   `module.py`: すべての処理モジュールの基底クラスとなる`_AkariModule`を定義します。モジュールが持つべき基本的なインターフェース（`call`, `stream_call`メソッドなど）を規定します。
    *   `router.py`: モジュールの実行とデータフローを管理する`_AkariRouter`を定義します。モジュール間の呼び出し、パラメータ渡し、ストリーミング処理の制御などを担当します。

2.  **`modules` ディレクトリ**: 具体的な機能を提供するモジュール群が格納されます。
    *   `__init__.py`: `modules`ディレクトリ内の主要なモジュールを公開します。
    *   **`audio` サブディレクトリ**: マイク入力（`mic.py`）やスピーカー出力（`speaker.py`）など、音声関連の機能を提供します。
    *   **`azure_openai` サブディレクトリ**: Azure OpenAI Serviceとの連携機能を提供します。大規模言語モデル（`llm.py`）、音声認識（`stt.py`）、音声合成（`tts.py`）のモジュールが含まれます。
    *   **`gemini` サブディレクトリ**: Google Geminiモデルとの連携機能を提供します（現在は`llm.py`のみ）。
    *   **`io` サブディレクトリ**: ファイル保存（`save.py`）など、入出力関連の機能を提供します。
    *   **`webrtcvad` サブディレクトリ**: WebRTC Voice Activity Detector（VAD）を用いた音声区間検出機能（`vad.py`）を提供します。
    *   `print.py`: デバッグ用にデータセットの内容をログに出力するモジュールです。
    *   `root.py`: パイプライン処理の起点となるルートモジュールです。最初に実行するモジュールを指定し、処理を開始します。
    *   `serial.py`: 複数のモジュールを定義された順序で逐次実行する機能を提供します。

3.  **`main.py`**: アプリケーションのエントリーポイントです。
    *   環境変数（`.env`ファイル）を読み込みます。
    *   必要なモジュールをインスタンス化し、ルーターに登録します。
    *   ルートモジュールを呼び出し、定義されたパイプライン処理を開始します。
    *   具体的な処理フロー（例: マイク入力 → VAD → STT → LLM → TTS → スピーカー出力）を定義します。

## モジュール間のインタラクション

*   `main.py`は、必要なモジュール群（`akari`コアモジュール、`modules`内の各種機能モジュール）を初期化し、`_AkariRouter`インスタンスに登録します。
*   処理の開始は、`main.py`が`_RootModule`の`call`メソッドを呼び出すことで行われます。
*   `_RootModule`は、パラメータとして渡された最初の処理モジュール（例: `_MicModule`）を`_AkariRouter`経由で呼び出します。
*   各モジュールは、`_AkariRouter`を介して次のモジュールを呼び出すことができます。これにより、モジュールが連鎖的に実行されるパイプラインが形成されます。
*   `_SerialModule`は、複数のモジュールを順番に実行するためのラッパーとして機能し、複雑な処理シーケンスを簡潔に定義できます。
*   モジュールは、処理結果を`AkariDataSet`オブジェクトに格納し、`AkariData`オブジェクトに追加して次のモジュールに渡します。
*   ストリーミング処理をサポートするモジュール（例: `_MicModule`, `_LLMModule`）は、部分的な結果をコールバックモジュールに非同期的に渡すことができます。

## 高レベルデータフロー

1.  **データ入力**: 通常、`_MicModule`のような入力モジュールが外部（マイク、ファイルなど）からデータを取得します。音声データの場合、PCM形式のバイトデータとして扱われます。
2.  **データ処理**:
    *   取得されたデータは`AkariDataSet`に格納され、`AkariData`オブジェクトに追加されます。
    *   `AkariData`オブジェクトは、パイプライン内の後続モジュールに順番に渡されます。
    *   各モジュールは、入力された`AkariData`から必要な情報（例: 前のモジュールの処理結果であるテキストデータや音声データ）を取り出し、自身の処理を実行します。
    *   例えば、VADモジュール（`_WebRTCVadModule`）は音声チャンクを受け取り、音声区間かどうかを判定します。STTモジュール（例: `_STTModule` on Azure）は音声バイトをテキストに変換します。LLMモジュール（例: `_LLMModule` on Azure or Gemini）はテキストプロンプトに基づいて応答テキストを生成します。TTSモジュール（例: `_TTSModule` on Azure）はテキストを音声バイトに変換します。
3.  **データ出力/連携**:
    *   処理結果のデータ（例: 生成された音声バイト）は、`_SpeakerModule`のような出力モジュールによって外部（スピーカー、ファイルなど）に出力されます。
    *   `_SaveModule`を使用して、パイプラインの任意の段階でデータをファイルに保存することも可能です。
    *   `_PrintModule`は、デバッグ目的でデータの内容をコンソールに出力します。
4.  **メタデータ**: 各`AkariDataSet`には、処理を実行したモジュールの種類、パラメータ、実行時間などのメタ情報が付与され、データのトレーサビリティを確保します。音声データの場合、サンプリングレートやチャンネル数などの情報もメタデータとして管理されます。

このように、Akariプロジェクトはモジュール化されたコンポーネントと明確なデータフローを通じて、柔軟なアプリケーション構築を実現します。
